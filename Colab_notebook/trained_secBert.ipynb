{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Document analysis with SecBERT trained classifier\n",
        "Mount your own drive space as working space with the following three commands"
      ],
      "metadata": {
        "id": "YLNgWkBH7bfY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9_sjQkPtInu",
        "outputId": "fa9c768c-bc4a-49fe-8570-0031dd7998ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zznG9RP5uyeK"
      },
      "outputs": [],
      "source": [
        "!cd '/content/drive/MyDrive/secBert'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkMJlgm5vy8O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/secBert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCNFnDgIzUBA"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip3 install torch torchvision\n",
        "!pip install transformers\n",
        "!pip install sklearn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni3y_xUEx1Q-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd \n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertConfig, AutoModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY0M0c3bym2j"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"jackaduma/SecBERT\")\n",
        "\n",
        "pretrained_model = AutoModelForMaskedLM.from_pretrained(\"jackaduma/SecBERT\")\n",
        "config = BertConfig.from_pretrained(\"jackaduma/SecBERT\", output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ksXmxn8gzCrk",
        "outputId": "04a825a5-44b3-422f-9a69-7b645313c044"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFjL8TF10eMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb8253d-9d18-46dc-da41-a2f507e44363"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "df = pd.read_csv('./dataset_new.csv')\n",
        "df = df.reset_index()\n",
        "\n",
        "df['sentence'] = df['sentence'].astype(str)\n",
        "LABELS = len(df['label_tec'].value_counts())\n",
        "\n",
        "#Encoding labels\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df['label_tec'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv('./train_dataset_tram.csv')\n",
        "train_dataset = train_dataset.reset_index()\n",
        "\n",
        "test_dataset = pd.read_csv('./testset_tram_x_ours.csv')\n",
        "test_dataset['enc_label'] = encoder.transform(test_dataset['label_tec'])\n",
        "test_dataset"
      ],
      "metadata": {
        "id": "jJx00WEaBcUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfb4CVpOpMIh"
      },
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvHKTeiCnWvo"
      },
      "outputs": [],
      "source": [
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        sentence = str(self.data.sentence[index])\n",
        "        sentence = \" \".join(sentence.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        if 'enc_label' not in self.data:\n",
        "            return {\n",
        "            'sentence': sentence,\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'sentence': sentence,\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.enc_label[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCYslNbiKSGb",
        "outputId": "6a8419b7-ff94-4e4a-9f3f-869de514df08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (12945, 6)\n",
            "TRAIN Dataset: (1185, 5)\n",
            "TEST Dataset: (294, 4)\n"
          ]
        }
      ],
      "source": [
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0 \n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKSA3ZUp2UJD"
      },
      "outputs": [],
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class SecBERTClass(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model_name: str, num_classes: int = None, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        config = BertConfig.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
        "        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config).base_model #pick only the main body of the model\n",
        "        #for param in self.model.parameters():\n",
        "          #param.requires_grad = False\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.classifier = torch.nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load previous trained model "
      ],
      "metadata": {
        "id": "4kdbooUB8gdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD\n",
        "model = SecBERTClass(\"jackaduma/SecBERT\", LABELS)\n",
        "model.load_state_dict(torch.load('trained_secbert.pt', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNmbUzUfYThE",
        "outputId": "ff2f5c31-e285-421a-ee11-1e2026884a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at jackaduma/SecBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "tagXkFeQYbER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQKBLLXr36jZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7c2c5f-e287-405e-a445-53986ebad485"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1185"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Function to calcuate the accuracy of the model\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "metadata": {
        "id": "3yI4O1ETi33T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO8rwAcRXY8Q"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model):\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    sentences = []\n",
        "    predicted = []\n",
        "    targets = []\n",
        "    predictions_arr = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(loader, 0):\n",
        "          x = data['ids'].to(device, dtype = torch.long)\n",
        "          mask = data['mask'].to(device, dtype = torch.long)\n",
        "          y = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "          scores = model(x, mask)\n",
        "          _, predictions = scores.max(1)\n",
        "          num_correct += (predictions == y).sum()\n",
        "          num_samples += predictions.size(0)\n",
        "\n",
        "          sentences += data['sentence']\n",
        "          predicted += scores\n",
        "          predictions_arr += predictions\n",
        "          targets += y\n",
        "\n",
        "      print(\n",
        "          f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
        "      )\n",
        "\n",
        "      return predicted, targets, sentences, predictions_arr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted, targets, sentences, predictions = check_accuracy(testing_loader, model)"
      ],
      "metadata": {
        "id": "0EQ3e_sPUceg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4ec8f6-ed96-43b3-bddb-cfa38cc68540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 137 / 294 with accuracy 46.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "id": "QM4tHLKqBN-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0] + ' ' + encoder.inverse_transform([predictions[0].item()]) + ' ' + encoder.inverse_transform([targets[0].item()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aPt-K92--HT",
        "outputId": "27726268-1b91-493b-8aac-a9c88edc123e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Additionally, a small number of campaigns over this same period also made use of various file-sharing platforms like Dropbox for hosting the malicious documents rather than directly attaching them to the messages themselves.Figure 2: Example malicious Excel documentSimilar to the technique described in our previous blog about Remcos, the contents of the documents have been intentionally made to appear as if they are blurry, with the user being prompted to enable editing to have a clearer view of the contents T1566 T1204']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0K6VGAGkF3q",
        "outputId": "92a30e4a-ed8b-4fad-cd4d-64806a83b587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 1878 / 2589 with accuracy 72.54\n"
          ]
        }
      ],
      "source": [
        "check_accuracy(testing_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM0wzVgZeVbO",
        "outputId": "c2375abc-03d2-46c6-be77-46f70584f0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyDeprecate==0.3.* in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy "
      ],
      "metadata": {
        "id": "NxhStrrHekwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchmetrics import F1Score, Precision, Recall, Accuracy\n",
        "f1 = F1Score(num_classes=LABELS)\n",
        "preds = torch.stack(predicted)\n",
        "tags = torch.tensor(targets)\n",
        "f1(preds, tags)"
      ],
      "metadata": {
        "id": "RCkF9bxSIvD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa069a71-96e4-4c42-8402-ed69f52c19c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4660)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = Precision(num_classes=LABELS)\n",
        "precision(preds,tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLc3YLURlyDJ",
        "outputId": "cb95969d-3777-4af9-e387-e5c05214d8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4660)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall = Recall(num_classes=LABELS)\n",
        "recall(preds, tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9lqm8AhmR-g",
        "outputId": "883c0231-9f33-422c-bd59-a30bfd2917ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4660)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = Accuracy(num_classes=LABELS, top_k=3)\n",
        "top_k(preds, tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83lr6Ty0nOxU",
        "outputId": "b1d1c99e-0072-4dbd-87f1-4ccf21cf08c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6531)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted"
      ],
      "metadata": {
        "id": "CXCPDb7MiQAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBi8fYC9OUBH"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def remove_empty_lines(text):\n",
        "\tlines = text.split(\"\\n\")\n",
        "\tnon_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
        "\n",
        "\tstring_without_empty_lines = \"\"\n",
        "\tfor line in non_empty_lines:\n",
        "\t\tif line != \"\\n\": \n",
        "\t\t\tstring_without_empty_lines += line + \"\\n\"\n",
        "\n",
        "\treturn string_without_empty_lines \n",
        "\n",
        "def combine_text(list_of_text):\n",
        "    combined_text = ' '.join(list_of_text)\n",
        "    return combined_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQPf5AX2fjCR",
        "outputId": "bf44d676-9c23-48c6-b45b-490463bc0d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import re\n",
        "\n",
        "def repl(matchobj):\n",
        "    return \",\"+ matchobj.group(1) + \",\"\n",
        "\n",
        "def load_regex(filename):\n",
        "    regex_list = []\n",
        "    with open(filename, 'r') as val:\n",
        "        document = yaml.safe_load(val)\n",
        "        regex_list = document\n",
        "    return regex_list\n",
        "\n",
        "def apply_regex_to_string(regex_list, string):\n",
        "    new_string = string\n",
        "    for rex in regex_list:\n",
        "        reg = rex.get('regex').strip()\n",
        "        raw_s = r'{}'.format(reg)\n",
        "        if re.search(raw_s, string):\n",
        "            new_string = re.sub(raw_s, rex.get('code') + \" \", string)\n",
        "            break\n",
        "    return new_string\n"
      ],
      "metadata": {
        "id": "ikzzq_8kisnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin6_tec_2 = ['T1134', 'T1059', 'T1562', 'T1036', 'T1588', 'T1003', 'T1021', 'T1569', 'T1078', 'T1102',\n",
        "        'T1087', 'T1482', 'T1069', 'T1018', 'T1016', 'T1548', 'T1071', 'T1185', 'T1059', \n",
        "        'T1543', 'T1132', 'T1005', 'T1001', 'T1140', 'T1573', 'T1068', 'T1083',\n",
        "        'T1564', 'T1562', 'T1070', 'T1105', 'T1056', 'T1112', 'T1046', 'T1095', 'T1027', 'T1137', \n",
        "        'T1003', 'T1069', 'T1057', 'T1055', 'T1572', 'T1572', 'T1090', 'T1012', 'T1620', 'T1021',\n",
        "        'T1018', 'T1029', 'T1113', 'T1518', 'T1553', 'T1218', 'T1049', 'T1007', 'T1569', 'T1550', \n",
        "        'T1078', 'T1047']\n",
        "\n",
        "fin6_tec_1 = ['T1087', 'T1560', 'T1119', 'T1547', 'T1110', 'T1059', 'T1074', 'T1573', 'T1068', 'T1070', \n",
        "        'T1046', 'T1003', 'T1572', 'T1021', 'T1018', 'T1053', 'T1078', 'T1003']\n",
        "\n",
        "# MenuPass [8]\n",
        "\n",
        "menuPass_tec_8 = ['T1560', 'T1119', 'T1059', 'T1005', 'T1074', 'T1210', 'T1083', 'T1574', 'T1106', 'T1027', 'T1003', 'T1199', 'T1078', 'T1047']\n",
        "\n",
        "adFind = ['T1087', 'T1482', 'T1069', 'T1018', 'T1016']\n",
        "\n",
        "certutil = ['T1140', 'T1105', 'T1553']\n",
        "\n",
        "quasarRAT = ['T1059', 'T1555', 'T1573', 'T1105', 'T1056', 'T1112', 'T1090', 'T1021', 'T1053', 'T1553', 'T1082', 'T1552', 'T1125']\n",
        "\n",
        "menuPass_tec_8.extend(adFind)\n",
        "menuPass_tec_8.extend(certutil)\n",
        "menuPass_tec_8.extend(quasarRAT)\n",
        "\n",
        "# MenuPass [2]\n",
        "\n",
        "menuPass_tec_2 = ['T1583', 'T1560', 'T1568', 'T1070', 'T1056', 'T1036', 'T1105', 'T1566', 'T1021', 'T1199', 'T1204', 'T1078']\n",
        "\n",
        "poisonIvy = ['T1010', 'T1547', 'T1059', 'T1543', 'T1005', 'T1074', 'T1573', 'T1105', 'T1056', 'T1112', 'T1027', \n",
        "'T1055', 'T1014']\n",
        "\n",
        "menuPass_tec_2.extend(poisonIvy)\n",
        "\n",
        "# WizardSpider [2]\n",
        "\n",
        "wizardSpider_tec_2 = ['T1547', 'T1059', 'T1562', 'T1135', 'T1566', 'T1055', 'T1021', 'T1053', 'T1558', 'T1204', 'T1047']\n",
        "\n",
        "bloodHound = ['T1087', 'T1560', 'T1059', 'T1482', 'T1615', 'T1106', 'T1201', 'T1069', 'T1018', 'T1033']\n",
        "\n",
        "cobaltStrike = ['T1548', 'T1134', 'T1087', 'T1071', 'T1197', 'T1185', 'T1059', 'T1043', 'T1543', 'T1132', 'T1005', 'T1001', 'T1030', 'T1140', 'T1573', 'T1203', 'T1068', 'T1083', 'T1564', 'T1562', 'T1070', 'T1105', 'T1056', 'T1112', 'T1026', 'T1106', 'T1046', 'T1135', 'T1095', 'T1027', 'T1137', 'T1003', 'T1069', 'T1057', 'T1055', 'T1572', 'T1090', 'T1012', 'T1620', 'T1021', 'T1018', 'T1029', 'T1113', 'T1518', 'T1553', 'T1218', 'T1016', 'T1049', 'T1007', 'T1569', 'T1550', 'T1078', 'T1047']\n",
        "\n",
        "empire =  ['T1548', 'T1134', 'T1087', 'T1557', 'T1071', 'T1560', 'T1547', 'T1217', 'T1115', 'T1059', 'T1043', 'T1136', 'T1543', 'T1555', 'T1484', 'T1482', 'T1114', 'T1573', 'T1546', 'T1068', 'T1083', 'T1574', 'T1210', 'T1615', 'T1567', 'T1070',  'T1056', 'T1105', 'T1056', 'T1106', 'T1046', 'T1135', 'T1040', 'T1027', 'T1003', 'T1057', 'T1055', 'T1021', 'T1053', 'T1113', 'T1518', 'T1558', 'T1082', 'T1016', 'T1049', 'T1569', 'T1127', 'T1552', 'T1550', 'T1125', 'T1102', 'T1047']\n",
        "\n",
        "mimikatz = ['T1134', 'T1098', 'T1547', 'T1555', 'T1003', 'T1207', 'T1558', 'T1552', 'T1550']\n",
        "\n",
        "ping = ['T1018']\n",
        "\n",
        "ryuk = ['T1134', 'T1547', 'T1059', 'T1486', 'T1083', 'T1222', 'T1562', 'T1490', 'T0828', 'T1036', 'T1106', 'T1027', 'T1057', 'T1055', 'T1021', 'T1053', 'T1489', 'T1082', 'T1614', 'T1016', 'T1205', 'T1078']\n",
        "\n",
        "trickBot = ['T1087', 'T1087', 'T1071', 'T1547', 'T1185', 'T1110', 'T1059', 'T1059', 'T1043', 'T1543', 'T1555', 'T1555', 'T1132', 'T1005', 'T1140', 'T1482', 'T1573', 'T1041', 'T1210', 'T1008', 'T1083', 'T1495', 'T1562', 'T1105', 'T1056', 'T1559', 'T1036', 'T1112', 'T1106', 'T1135', 'T1571', 'T1027', 'T1027', 'T1069', 'T1566', 'T1566', 'T1542', 'T1057', 'T1055', 'T1055', 'T1090', 'T1219', 'T1021', 'T1018', 'T1053', 'T1553', 'T1082', 'T1016', 'T1033', 'T1007', 'T1552', 'T1552', 'T1204', 'T1497']\n",
        "\n",
        "wizardSpider_tec_2.extend(bloodHound)\n",
        "wizardSpider_tec_2.extend(cobaltStrike)\n",
        "wizardSpider_tec_2.extend(empire)\n",
        "wizardSpider_tec_2.extend(mimikatz)\n",
        "wizardSpider_tec_2.extend(ping)\n",
        "wizardSpider_tec_2.extend(ryuk)\n",
        "wizardSpider_tec_2.extend(trickBot)\n",
        "\n",
        "#WizardSpider [7]\n",
        "\n",
        "wizardSpider_tec_7 = ['T1087', 'T1059', 'T1048', 'T1210', 'T1562', 'T1027', 'T1021', 'T1018', 'T1489', 'T1518', 'T1558', 'T1082', 'T1569']\n",
        "\n",
        "adFind = ['T1087', 'T1482', 't1069', 'T1018', 'T1016']\n",
        "\n",
        "#CobaltStrike\n",
        "\n",
        "net = ['T1087', 'T1087', 'T1136', 'T1136', 'T1070', 'T1135', 'T1201', 'T1069', 'T1069', 'T1021', 'T1018', 'T1049', 'T1007', 'T1569', 'T1124']\n",
        "\n",
        "nltest = ['T1482', 'T1018', 'T1016']\n",
        "\n",
        "#Ping\n",
        "\n",
        "#Ryuk\n",
        "\n",
        "wizardSpider_tec_7.extend(adFind)\n",
        "wizardSpider_tec_7.extend(cobaltStrike)\n",
        "wizardSpider_tec_7.extend(net)\n",
        "wizardSpider_tec_7.extend(nltest)\n",
        "wizardSpider_tec_7.extend(ping)\n",
        "wizardSpider_tec_7.extend(ryuk)"
      ],
      "metadata": {
        "id": "ncpuFFwkknfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin6_files = ['documents/FIN6/Follow The Money-Dissecting the Operations of the Cyber Crime Group FIN6[1].txt', \n",
        "                'documents/FIN6/Pick-Six-Intercepting a FIN6 Intrusion, an Actor Recently Tied to Ryuk and LockerGoga Ransomware[2].txt',\n",
        "                'documents/FIN6/intelligence_summary.txt']\n",
        "\n",
        "menuPass_files = ['documents/MenuPass/2018_12_20_united_states_v_zhu_hua_indictment[2].txt', \n",
        "                'documents/MenuPass/Japan-Linked Organizations Targeted in Long-Running and Sophisticated Attack Campaign[8].txt']\n",
        "\n",
        "wizardSpider_files = ['documents/WizardSpider/Ryuk’s Return[7].txt',\n",
        "                     'documents/WizardSpider/Ransomware Activity Targeting the Healthcare and Public Health Sector. Retrieved October 28, 2020[2].txt']"
      ],
      "metadata": {
        "id": "8LS7N0SClZ26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = wizardSpider_files[1]\n",
        "techniques = wizardSpider_tec_2"
      ],
      "metadata": {
        "id": "HqaSVFi6lmq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgMYAc4ZTqKb"
      },
      "outputs": [],
      "source": [
        "#Read report text from txt file\n",
        "lines = []\n",
        "file_paths = [file_name]\n",
        "for file_path in file_paths:\n",
        "    with open(file_path) as f:\n",
        "        lines += f.readlines()\n",
        "import re\n",
        "## Apply regex \n",
        "regex_list = load_regex(\"regex.yml\")\n",
        "\n",
        "text = combine_text(lines)\n",
        "text = re.sub('(%(\\w+)%(\\/[^\\s]+))', repl, text)\n",
        "text = apply_regex_to_string(regex_list, text)\n",
        "text = re.sub('\\(.*?\\)', '', text)\n",
        "text = remove_empty_lines(text)\n",
        "text = text.strip()\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "double_sentences = []\n",
        "\n",
        "for i in range(1, len(sentences)):\n",
        "    new_sen = sentences[i-1] + sentences[i]\n",
        "    double_sentences.append(new_sen)\n",
        " \n",
        "data = {'sentence': sentences}\n",
        "df = pd.DataFrame(data, columns=['sentence'])\n",
        "sentence_set = Triage(df, tokenizer, MAX_LEN)\n",
        "testing_loader = DataLoader(sentence_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7BoH3TLod7u",
        "outputId": "aca84e86-2ca6-4797-821c-62303b901f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "275"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9sTl5jnPJAR"
      },
      "outputs": [],
      "source": [
        "predicted = []\n",
        "predict_proba_scores = []\n",
        "with torch.no_grad():\n",
        "      for i, data in enumerate(testing_loader, 0):\n",
        "          x = data['ids'].to(device, dtype = torch.long)\n",
        "          mask = data['mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          scores = model(x, mask)\n",
        "          _, predictions = scores.max(1)\n",
        "          proba_scores = torch.nn.functional.softmax(scores, dim=1)\n",
        "\n",
        "          predicted += predictions\n",
        "          predict_proba_scores += proba_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [ pred.item() for pred in predicted]\n",
        "predicted"
      ],
      "metadata": {
        "id": "RspfNYDBsI5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_proba_scores = [pred.max().item() for pred in predict_proba_scores]\n",
        "predict_proba_scores"
      ],
      "metadata": {
        "id": "pxTyEwIWoVVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_proba_scores"
      ],
      "metadata": {
        "id": "TWMpOf2JncPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = encoder.inverse_transform(predicted) \n",
        "predicted"
      ],
      "metadata": {
        "id": "ZBh7wZ2atW-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_measure(recall, precision):\n",
        "    if recall != 0 and precision != 0:\n",
        "        return (2*precision*recall)/(precision+recall)\n",
        "    else:\n",
        "        return 0.01"
      ],
      "metadata": {
        "id": "l1rfPCt5SQ4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRxPc6noPOvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e688e8-9afb-47e8-f8d0-2ba717b5fd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "275\n",
            "['T1102' 'T1102' 'T1071' 'T1135' 'T1189' 'T1057' 'T1070' 'T1102' 'T1036'\n",
            " 'T1027' 'T1562' 'T1102' 'T1486' 'T1591' 'T1059' 'T1102' 'T1055' 'T1204'\n",
            " 'T1486' 'T1218' 'T1102' 'T1574' 'T1140' 'T1218' 'T1102' 'T1588' 'T1550'\n",
            " 'T1102' 'T1102' 'T1562' 'T1542' 'T1102' 'T1046' 'T1102' 'T1592' 'T1497'\n",
            " 'T1102' 'T1553' 'T1562' 'T1102' 'T1562' 'T1547' 'T1189' 'T1070' 'T1074'\n",
            " 'T1102' 'T1102' 'T1018' 'T1587' 'T1195' 'T1566' 'T1505' 'T1036' 'T1036'\n",
            " 'T1562' 'T1102' 'T1001' 'T1046' 'T1102' 'T1102' 'T1046' 'T1048' 'T1583'\n",
            " 'T1102' 'T1486' 'T1218' 'T1120' 'T1071' 'T1562' 'T1218' 'T1218' 'T1102'\n",
            " 'T1608' 'T1587' 'T1480' 'T1486' 'T1102' 'T1592' 'T1027' 'T1102' 'T1102'\n",
            " 'T1140' 'T1102' 'T1496' 'T1102' 'T1012' 'T1036' 'T1102' 'T1566' 'T1566'\n",
            " 'T1137' 'T1029' 'T1059' 'T1027' 'T1486' 'T1021' 'T1102' 'T1102' 'T1102'\n",
            " 'T1496' 'T1102' 'T1016' 'T1102' 'T1542' 'T1102' 'T1102' 'T1070' 'T1001'\n",
            " 'T1003' 'T1102' 'T1137' 'T1486' 'T1490' 'T1480' 'T1016' 'T1592' 'T1106'\n",
            " 'T1486' 'T1484' 'T1018' 'T1486' 'T1102' 'T1542' 'T1135' 'T1562' 'T1102'\n",
            " 'T1562' 'T1566' 'T1102' 'T1486' 'T1189' 'T1102' 'T1027' 'T1104' 'T1053'\n",
            " 'T1102' 'T1140' 'T1046' 'T1102' 'T1562' 'T1490' 'T1110' 'T1036' 'T1124'\n",
            " 'T1490' 'T1027' 'T1505' 'T1480' 'T1102' 'T1480' 'T1070' 'T1102' 'T1589'\n",
            " 'T1105' 'T1542' 'T1102' 'T1102' 'T1074' 'T1102' 'T1218' 'T1018' 'T1542'\n",
            " 'T1102' 'T1036' 'T1016' 'T1102' 'T1564' 'T1027' 'T1102' 'T1562' 'T1102'\n",
            " 'T1001' 'T1102' 'T1102' 'T1588' 'T1027' 'T1518' 'T1027' 'T1102' 'T1102'\n",
            " 'T1102' 'T1489' 'T1498' 'T1189' 'T1021' 'T1027' 'T1218' 'T1489' 'T1592'\n",
            " 'T1053' 'T1102' 'T1018' 'T1102' 'T1036' 'T1027' 'T1102' 'T1036' 'T1573'\n",
            " 'T1486' 'T1218' 'T1102' 'T1102' 'T1102' 'T1102' 'T1053' 'T1102' 'T1562'\n",
            " 'T1102' 'T1074' 'T1102' 'T1562' 'T1027' 'T1489' 'T1566' 'T1552' 'T1568'\n",
            " 'T1484' 'T1102' 'T1564' 'T1016' 'T1591' 'T1049' 'T1027' 'T1567' 'T1587'\n",
            " 'T1134' 'T1518' 'T1547' 'T1102' 'T1102' 'T1090' 'T1078' 'T1102' 'T1001'\n",
            " 'T1046' 'T1008' 'T1587' 'T1562' 'T1486' 'T1082' 'T1102' 'T1102' 'T1102'\n",
            " 'T1480' 'T1102' 'T1083' 'T1102' 'T1556' 'T1484' 'T1102' 'T1566' 'T1102'\n",
            " 'T1484' 'T1057' 'T1480' 'T1036' 'T1036' 'T1486' 'T1588' 'T1102' 'T1102'\n",
            " 'T1564' 'T1070' 'T1106' 'T1562' 'T1074' 'T1027' 'T1001' 'T1102' 'T1218'\n",
            " 'T1036' 'T1102' 'T1102' 'T1027' 'T1587']\n"
          ]
        }
      ],
      "source": [
        "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "precisions = []\n",
        "recalls = []\n",
        "corrected_pred = []\n",
        "accepted_pred = []\n",
        "correct_on_uniques = []\n",
        "f1s = []\n",
        "\n",
        "print(len(predicted))\n",
        "print(predicted)\n",
        "\n",
        "lines = len(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for threshold in thresholds: \n",
        "    tecs = set(techniques)\n",
        "    accepted = []\n",
        "\n",
        "    for i in range(0,len(predict_proba_scores)):\n",
        "        top_class = predicted[i]\n",
        "        proba = predict_proba_scores[i]\n",
        "        if proba > threshold:\n",
        "            accepted.append(top_class)\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    unique_accepted = set(accepted)\n",
        "\n",
        "    len_tecs = len(tecs)\n",
        "\n",
        "    for pred in accepted:\n",
        "        if pred in tecs: #True Positives\n",
        "            correct += 1\n",
        "    print(len(accepted))\n",
        "    print(correct)\n",
        "\n",
        "    if len(accepted) != 0:\n",
        "        precision = correct/len(accepted)*100\n",
        "    else:\n",
        "        precision = 0\n",
        "    \n",
        "    precision = round(precision,2)\n",
        "    print(precision) #accuracy or precision?\n",
        "\n",
        "    precisions.append(precision)\n",
        "\n",
        "    for pred in accepted:\n",
        "        if pred in tecs:\n",
        "            tecs.remove(pred)\n",
        "\n",
        "    recall = str(len_tecs-len(tecs))+ '/' + str(len_tecs)\n",
        "\n",
        "    print(recall) #Recall\n",
        "\n",
        "    recalls.append(recall)\n",
        "    recall = (len_tecs-len(tecs))/len_tecs\n",
        "\n",
        "    corrected_pred.append(correct) \n",
        "    accepted_pred.append(len(accepted))\n",
        "    \n",
        "    cou = str(len_tecs-len(tecs))+ '/' + str(len(unique_accepted))\n",
        "    correct_on_uniques.append(cou)\n",
        "    cou = 0 if len(unique_accepted) == 0 else (len_tecs-len(tecs))/len(unique_accepted)\n",
        "\n",
        "    f1 = f_measure(recall=recall, precision=cou)\n",
        "    f1 = round(f1,2)\n",
        "    f1s.append(f1)\n",
        "\n",
        "    print(\"Threshold: \" + str(threshold) + \": \" + str(cou) + \" correct on uniques\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WObvAx2tRvpY",
        "outputId": "07ee6850-b285-4cb9-d6dd-933db1f79b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240\n",
            "211\n",
            "87.92\n",
            "47/99\n",
            "Threshold: 0.1: 0.7580645161290323 correct on uniques\n",
            "139\n",
            "127\n",
            "91.37\n",
            "40/99\n",
            "Threshold: 0.2: 0.8333333333333334 correct on uniques\n",
            "111\n",
            "105\n",
            "94.59\n",
            "37/99\n",
            "Threshold: 0.3: 0.9024390243902439 correct on uniques\n",
            "93\n",
            "88\n",
            "94.62\n",
            "32/99\n",
            "Threshold: 0.4: 0.8888888888888888 correct on uniques\n",
            "80\n",
            "78\n",
            "97.5\n",
            "30/99\n",
            "Threshold: 0.5: 0.967741935483871 correct on uniques\n",
            "64\n",
            "62\n",
            "96.88\n",
            "29/99\n",
            "Threshold: 0.6: 0.9666666666666667 correct on uniques\n",
            "57\n",
            "56\n",
            "98.25\n",
            "28/99\n",
            "Threshold: 0.7: 0.9655172413793104 correct on uniques\n",
            "44\n",
            "43\n",
            "97.73\n",
            "24/99\n",
            "Threshold: 0.8: 0.96 correct on uniques\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier_results:\n",
        "    def __init__(self, title, lines, accepted_preds, correct_preds, precisions, recalls, correct_uniques, f1s):\n",
        "        self.title = title\n",
        "        self.lines = lines\n",
        "        self.accepted_preds = accepted_preds\n",
        "        self.correct_preds = correct_preds\n",
        "        self.precisions = precisions\n",
        "        self.recalls = recalls\n",
        "        self.correct_uniques = correct_uniques\n",
        "        self.f1s = f1s\n",
        "\n",
        "class CSVOutput:\n",
        "    def __init__(self, document_title, classifiers):\n",
        "        self.classifiers = classifiers\n",
        "        self.document_title = document_title\n",
        "\n",
        "    def printify_array(self, array, sep = ';'):\n",
        "        return sep + sep.join(str(x) for x in array)\n",
        "\n",
        "    def _save_classifier_outputs(self, f):\n",
        "        for classifier in self.classifiers:\n",
        "            f.write(classifier.title + '\\n')\n",
        "            f.write(str(classifier.lines) + ' sentences\\n')\n",
        "            f.write('Accepted Predictions: {}\\n'.format(self.printify_array(classifier.accepted_preds)))\n",
        "            f.write('Corrected Predictions: {}\\n'.format(self.printify_array(classifier.correct_preds)))\n",
        "            f.write('Precision%: {}\\n'.format(self.printify_array(classifier.precisions)))\n",
        "            f.write('Recall%: {}\\n'.format(self.printify_array(classifier.recalls)))\n",
        "            f.write('Correct predictions on uniques: {}\\n\\n'.format(self.printify_array(classifier.correct_uniques)))\n",
        "\n",
        "    def _save_classifier_f1(self, path):\n",
        "        with open(path+'/'+self.document_title+'_f1.txt', 'w') as f:\n",
        "            for classifier in self.classifiers:\n",
        "                f.write(classifier.title)\n",
        "                f.write(self.printify_array(classifier.f1s)+ '\\n')\n",
        "\n",
        "    def write_to_file(self, path):\n",
        "        self._save_classifier_f1(path)\n",
        "        with open(path+'/'+self.document_title+'.csv', 'w') as f:\n",
        "            f.write('Tresholds; 0,1; 0.2; 0.3; 0.4; 0.5; 0.6; 0.7; 0.8;\\n')\n",
        "            self._save_classifier_outputs(f)\n",
        "            \n",
        "    def append_to_file(self, path):\n",
        "        self._save_classifier_f1(path)\n",
        "        with open(path+'/'+self.document_title+'.csv', 'a') as f:\n",
        "            self._save_classifier_outputs(f)"
      ],
      "metadata": {
        "id": "_u0fSkSoS2ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = Classifier_results( title='secBert', \n",
        "                              lines=lines,\n",
        "                              accepted_preds=accepted_pred, \n",
        "                              correct_preds=corrected_pred, \n",
        "                              precisions=precisions, \n",
        "                              recalls=recalls, \n",
        "                              correct_uniques=correct_on_uniques,\n",
        "                              f1s=f1s)"
      ],
      "metadata": {
        "id": "tCGJkAEmSjqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin6_ref_1_output = CSVOutput('documents/wizardSpider_ref_2', [result])\n",
        "fin6_ref_1_output.write_to_file('.')"
      ],
      "metadata": {
        "id": "ZKTyX1TUUnnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "trained_secBert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}